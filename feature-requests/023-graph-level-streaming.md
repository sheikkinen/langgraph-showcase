# Feature Request: Graph-Level Streaming

**Priority:** HIGH
**Type:** Feature
**Status:** Proposed
**Effort:** 1-2 days
**Requested:** 2026-02-10

## Summary

Add native streaming support to graph execution so LLM nodes can yield tokens through the compiled graph pipeline, enabling real-time SSE output without bypassing the graph.

## Problem

### Violated Objective

YAMLGraph's proxy example (`examples/openai_proxy/`) cannot natively stream LLM responses through the graph pipeline. The current workaround runs the full graph synchronously, then **fakes** streaming by splitting the completed response into words and dripping them through SSE at 50ms intervals.

### Traces

- **Fake streaming deployed**: commit `ce2a7c2` — `examples/openai_proxy/api/app.py` lines 116-155
- **Real streaming exists at prompt level**: `yamlgraph/executor_async.py:131` — `execute_prompt_streaming()` uses `llm.astream()` but operates outside the graph pipeline
- **3rd party integration**: Live at `yamlgraph-proxy.fly.dev`, serving `AsyncOpenAI/Python 2.6.1` clients that send `"stream": true`
- **Streaming demo proves the primitive works**: `examples/demos/streaming/demo_streaming.py` — 123 chunks, real token-by-token output via `execute_prompt_streaming()`

### Architectural Gap

```
Current (fake):
  request → [echo] → [validate] → [respond: FULL WAIT] → split words → SSE drip
  TTFT = total generation time (2-10s)

Desired (real):
  request → [echo] → [validate] → [respond: token₁→SSE, token₂→SSE, ...] → [DONE]
  TTFT ≈ 200ms
```

The graph executor (`compiled.invoke()`) is all-or-nothing. There is no `compiled.astream()` that yields intermediate results from individual nodes. To get real streaming, the proxy has to bypass the framework — which undermines its value as an example.

### Why This Matters

Option A (hybrid: call Python nodes directly, then `execute_prompt_streaming()`) was considered and rejected. It bypasses `graph.yaml`, making the example an indictment of yamlgraph rather than a showcase. If the example has to work around the framework, the framework has a gap.

## Proposed Solution

### New API: `run_graph_streaming()`

```python
# yamlgraph/executor_async.py

async def run_graph_streaming(
    graph_path: str,
    initial_state: dict,
    stream_node: str | None = None,
) -> AsyncIterator[str]:
    """Execute a graph, streaming tokens from the LLM node.

    Runs all non-LLM nodes synchronously, then streams the
    designated LLM node's output token-by-token.

    Args:
        graph_path: Path to graph YAML
        initial_state: Initial state dict
        stream_node: Name of the node to stream (auto-detected if None;
                     uses the last LLM node in the pipeline)

    Yields:
        Token strings as they are generated by the LLM node
    """
```

### Graph YAML: `stream: true`

```yaml
nodes:
  echo:
    type: python
    tool: echo_input

  validate:
    type: python
    tool: validate_input

  respond:
    type: llm
    prompt: respond
    state_key: response
    stream: true  # ← enables token-by-token streaming
```

### Proxy Integration

```python
# examples/openai_proxy/api/app.py

if request.stream:
    async def stream_response():
        async for token in run_graph_streaming(
            graph_path=os.getenv("GRAPH_PATH", "examples/openai_proxy/graph.yaml"),
            initial_state={"input": messages_json},
        ):
            chunk = format_sse_chunk(chat_id, created, request.model, token)
            yield f"data: {json.dumps(chunk)}\n\n"
        yield f"data: {json.dumps(done_chunk)}\n\n"
        yield "data: [DONE]\n\n"

    return StreamingResponse(stream_response(), media_type="text/event-stream")
```

### Implementation Sketch

```python
async def run_graph_streaming(graph_path, initial_state, stream_node=None):
    from yamlgraph.graph_loader import load_and_compile

    state_graph = load_and_compile(graph_path)
    graph_config = state_graph.config  # Access parsed YAML config

    # Identify nodes and their types
    nodes = graph_config["nodes"]
    if stream_node is None:
        # Auto-detect: last LLM node in edge order
        stream_node = next(
            name for name, cfg in reversed(nodes.items())
            if cfg.get("type") == "llm"
        )

    # 1. Run all nodes BEFORE the stream node synchronously
    compiled = state_graph.compile()
    # Execute partial graph up to (but not including) the stream_node
    # This requires either:
    #   a) Running nodes individually via their functions, or
    #   b) Using LangGraph's interrupt mechanism to pause before stream_node

    # 2. Stream the LLM node using execute_prompt_streaming()
    node_config = nodes[stream_node]
    prompt_name = node_config["prompt"]
    state_key = node_config.get("state_key", "response")

    async for token in execute_prompt_streaming(
        prompt_name,
        variables=pre_stream_state,
        provider=os.getenv("PROVIDER"),
        prompts_dir=graph_config.get("defaults", {}).get("prompts_dir"),
        prompts_relative=graph_config.get("defaults", {}).get("prompts_relative", False),
        graph_path=Path(graph_path),
    ):
        yield token
```

## Acceptance Criteria

- [ ] `run_graph_streaming()` in `yamlgraph/executor_async.py`
- [ ] `stream: true` recognized in `graph.yaml` node config
- [ ] Python nodes before the stream node execute normally
- [ ] LLM node streams tokens via async generator
- [ ] Proxy example uses `run_graph_streaming()` instead of fake streaming
- [ ] State is correctly populated by pre-stream nodes (echo, validate)
- [ ] Error mid-stream propagates cleanly
- [ ] Tests: unit tests for `run_graph_streaming()` with mocked LLM
- [ ] Tests: SSE format compliance (OpenAI chunk schema)
- [ ] Requirement `REQ-YG-0XX` added to `ARCHITECTURE.md`
- [ ] Tests tagged with `@pytest.mark.req("REQ-YG-0XX")`
- [ ] `req-coverage.py` updated
- [ ] Documentation in `reference/streaming.md` updated
- [ ] Streaming demo updated or new demo added

## Alternatives Considered

| Option | Verdict |
|--------|---------|
| **A: Hybrid (call tools directly + `execute_prompt_streaming`)** | Rejected — bypasses `graph.yaml`, undermines the example |
| **C: Direct LLM (skip guardrails)** | Rejected — removes yamlgraph from the picture entirely |
| **Fake streaming (current)** | Stopgap — works but TTFT = total generation time |

## Related

- `yamlgraph/executor_async.py` — `execute_prompt_streaming()`, `run_graph_async()`
- `examples/openai_proxy/api/app.py` — Current fake streaming (commit `ce2a7c2`)
- `examples/demos/streaming/demo_streaming.py` — Working prompt-level streaming demo
- `reference/streaming.md` — Streaming documentation
- `docs-planning/plan-streaming-proxy.md` — Detailed analysis of all three options
- LangGraph `astream_events` — potential upstream primitive to leverage
