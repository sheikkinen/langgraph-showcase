"""Async Prompt Executor - Async interface for LLM calls.

This module provides async versions of execute_prompt for use in
async contexts like web servers or concurrent pipelines.

Note: This is a foundation module. The underlying LLM calls still
use sync HTTP clients wrapped with run_in_executor.
"""

from __future__ import annotations

import asyncio
import logging
from collections.abc import AsyncIterator
from pathlib import Path
from typing import TYPE_CHECKING, TypeVar

from pydantic import BaseModel

from yamlgraph.config import DEFAULT_TEMPERATURE
from yamlgraph.executor_base import prepare_messages
from yamlgraph.utils.llm_factory import create_llm
from yamlgraph.utils.llm_factory_async import invoke_async

if TYPE_CHECKING:
    from langgraph.graph.state import CompiledStateGraph

logger = logging.getLogger(__name__)

T = TypeVar("T", bound=BaseModel)


async def execute_prompt_async(
    prompt_name: str,
    variables: dict | None = None,
    output_model: type[T] | None = None,
    temperature: float = DEFAULT_TEMPERATURE,
    provider: str | None = None,
    graph_path: Path | None = None,
    prompts_dir: Path | None = None,
    prompts_relative: bool = False,
    state: dict | None = None,
) -> T | str:
    """Execute a YAML prompt asynchronously.

    Async version of execute_prompt for use in async contexts.

    Args:
        prompt_name: Name of the prompt file (without .yaml)
        variables: Variables to substitute in the template
        output_model: Optional Pydantic model for structured output
        temperature: LLM temperature setting
        provider: LLM provider ("anthropic", "mistral", "openai")
        graph_path: Path to graph file for relative prompt resolution
        prompts_dir: Explicit prompts directory override
        prompts_relative: If True, resolve prompts relative to graph_path
        state: Optional state dict for Jinja2 templates (accessible as {{ state.field }})

    Returns:
        Parsed Pydantic model if output_model provided, else raw string

    Example:
        >>> result = await execute_prompt_async(
        ...     "greet",
        ...     variables={"name": "World"},
        ...     output_model=GenericReport,
        ... )
    """
    messages, resolved_provider, resolved_model = prepare_messages(
        prompt_name=prompt_name,
        variables=variables,
        provider=provider,
        graph_path=graph_path,
        prompts_dir=prompts_dir,
        prompts_relative=prompts_relative,
        state=state,
    )

    # Create LLM (cached via factory)
    llm = create_llm(
        temperature=temperature, provider=resolved_provider, model=resolved_model
    )

    # Invoke asynchronously
    return await invoke_async(llm, messages, output_model)


async def execute_prompts_concurrent(
    prompts: list[dict],
) -> list[BaseModel | str]:
    """Execute multiple prompts concurrently.

    Useful for parallel LLM calls in pipelines.

    Args:
        prompts: List of dicts with keys:
            - prompt_name: str (required)
            - variables: dict (optional)
            - output_model: Type[BaseModel] (optional)
            - temperature: float (optional)
            - provider: str (optional)

    Returns:
        List of results in same order as input prompts

    Example:
        >>> results = await execute_prompts_concurrent([
        ...     {"prompt_name": "summarize", "variables": {"text": "..."}},
        ...     {"prompt_name": "analyze", "variables": {"text": "..."}},
        ... ])
    """
    tasks = []
    for prompt_config in prompts:
        task = execute_prompt_async(
            prompt_name=prompt_config["prompt_name"],
            variables=prompt_config.get("variables"),
            output_model=prompt_config.get("output_model"),
            temperature=prompt_config.get("temperature", DEFAULT_TEMPERATURE),
            provider=prompt_config.get("provider"),
        )
        tasks.append(task)

    return await asyncio.gather(*tasks)


# ==============================================================================
# Streaming Support (Phase 3 - Feature 004)
# ==============================================================================


async def execute_prompt_streaming(
    prompt_name: str,
    variables: dict | None = None,
    temperature: float = DEFAULT_TEMPERATURE,
    provider: str | None = None,
    graph_path: Path | None = None,
    prompts_dir: Path | None = None,
    prompts_relative: bool = False,
    state: dict | None = None,
) -> AsyncIterator[str]:
    """Execute a YAML prompt with streaming token output.

    Yields tokens as they are generated by the LLM. Does not support
    structured output (output_model) - use execute_prompt_async for that.

    Args:
        prompt_name: Name of the prompt file (without .yaml)
        variables: Variables to substitute in the template
        temperature: LLM temperature setting
        provider: LLM provider ("anthropic", "mistral", "openai")
        graph_path: Path to graph file for relative prompt resolution
        prompts_dir: Explicit prompts directory override
        prompts_relative: If True, resolve prompts relative to graph_path
        state: Optional state dict for Jinja2 templates (accessible as {{ state.field }})

    Yields:
        Token strings as they are generated

    Example:
        >>> async for token in execute_prompt_streaming("greet", {"name": "World"}):
        ...     print(token, end="", flush=True)
        Hello, World!
    """
    messages, resolved_provider, resolved_model = prepare_messages(
        prompt_name=prompt_name,
        variables=variables,
        provider=provider,
        graph_path=graph_path,
        prompts_dir=prompts_dir,
        prompts_relative=prompts_relative,
        state=state,
    )

    # Create LLM (cached via factory)
    llm = create_llm(
        temperature=temperature, provider=resolved_provider, model=resolved_model
    )

    # Stream tokens
    async for chunk in llm.astream(messages):
        content = chunk.content
        if content:  # Skip empty chunks
            yield content


# ==============================================================================
# Async Graph Execution (Phase 2 - Feature 003)
# ==============================================================================


async def run_graph_async(
    app,
    initial_state: dict,
    config: dict | None = None,
) -> dict:
    """Execute a compiled graph asynchronously.

    Thin wrapper around LangGraph's ainvoke for consistent API.
    Supports interrupt handling and Command resume.

    Args:
        app: Compiled LangGraph app (from graph.compile())
        initial_state: Initial state dict or Command(resume=...) for resuming
        config: LangGraph config with thread_id, e.g.
                {"configurable": {"thread_id": "my-thread"}}

    Returns:
        Final state dict. If interrupted, contains "__interrupt__" key.

    Example:
        >>> app = load_and_compile_async("graphs/interview.yaml")
        >>> result = await run_graph_async(
        ...     app,
        ...     {"query": "hello"},
        ...     {"configurable": {"thread_id": "t1"}},
        ... )
        >>> if "__interrupt__" in result:
        ...     # Handle interrupt - get user input
        ...     result = await run_graph_async(
        ...         app,
        ...         Command(resume="user answer"),
        ...         {"configurable": {"thread_id": "t1"}},
        ...     )
    """
    config = config or {}
    return await app.ainvoke(initial_state, config)


async def compile_graph_async(
    graph,
    config,
) -> CompiledStateGraph:
    """Compile a StateGraph with async-compatible checkpointer.

    Uses get_checkpointer_async() to properly initialize async Redis savers.

    Args:
        graph: StateGraph instance
        config: GraphConfig with optional checkpointer field

    Returns:
        Compiled graph ready for ainvoke()
    """
    from yamlgraph.storage.checkpointer_factory import get_checkpointer_async

    checkpointer_config = getattr(config, "checkpointer", None)
    checkpointer = await get_checkpointer_async(checkpointer_config)

    return graph.compile(checkpointer=checkpointer)


async def load_and_compile_async(path: str) -> CompiledStateGraph:
    """Load YAML and compile to async-ready graph.

    Convenience function combining load_graph_config, compile_graph,
    and compile_graph_async.

    Args:
        path: Path to YAML graph definition

    Returns:
        Compiled graph ready for ainvoke()

    Example:
        >>> app = await load_and_compile_async("graphs/interview.yaml")
        >>> result = await run_graph_async(app, {"input": "hi"}, config)
    """
    from yamlgraph.graph_loader import compile_graph, load_graph_config

    config = load_graph_config(path)
    logger.info(f"Loaded graph config: {config.name} v{config.version}")

    state_graph = compile_graph(config)
    return await compile_graph_async(state_graph, config)


# ==============================================================================
# Graph-Level Streaming (FR-023 - REQ-YG-048)
# ==============================================================================

# Non-LLM node types that should run before streaming
_NON_LLM_TYPES = frozenset(
    {
        "python",
        "tool",
        "passthrough",
        "map",
        "interrupt",
        "subgraph",
        "tool_call",
        "agent",
    }
)


async def run_graph_streaming(
    graph_path: str,
    initial_state: dict,
) -> AsyncIterator[str]:
    """Execute a graph with streaming output from the LLM node.

    Loads the graph, runs non-LLM nodes (python, tool, etc.) to build
    state, then streams the LLM node's output token by token.

    Strategy:
        1. Deep-copy the graph config with the LLM node set to passthrough
        2. Compile and run the modified graph to execute pre-processing nodes
        3. Stream the LLM node via execute_prompt_streaming()

    Args:
        graph_path: Path to graph YAML file
        initial_state: Initial state dict

    Yields:
        Token strings as they are generated by the LLM

    Example:
        >>> async for token in run_graph_streaming(
        ...     "examples/openai_proxy/graph.yaml",
        ...     {"input": "Hello!"},
        ... ):
        ...     print(token, end="", flush=True)
    """
    import copy

    from yamlgraph.graph_loader import compile_graph, load_graph_config
    from yamlgraph.utils.expressions import resolve_node_variables

    config = load_graph_config(graph_path)
    path = Path(graph_path)
    defaults = config.defaults or {}

    # Find the LLM node (first node whose type is not in _NON_LLM_TYPES)
    llm_node_name = None
    llm_node_config = None
    for name, node_cfg in config.nodes.items():
        node_type = node_cfg.get("type", "llm")
        if node_type not in _NON_LLM_TYPES:
            llm_node_name = name
            llm_node_config = node_cfg
            break

    if not llm_node_name:
        logger.warning("No LLM node found in graph %s â€” nothing to stream", graph_path)
        return

    # Build modified graph with LLM node as passthrough
    config_pre = copy.deepcopy(config)
    config_pre.nodes[llm_node_name]["type"] = "passthrough"

    graph = compile_graph(config_pre)
    compiled = graph.compile()

    # Run pre-processing nodes to build state
    pre_state = await compiled.ainvoke(initial_state)

    # Resolve variables for the LLM node
    variable_templates = llm_node_config.get("variables", {})
    variables = resolve_node_variables(variable_templates, pre_state)

    # Resolve LLM node parameters
    prompt_name = llm_node_config.get("prompt", llm_node_name)
    temperature = llm_node_config.get(
        "temperature", defaults.get("temperature", DEFAULT_TEMPERATURE)
    )
    provider = llm_node_config.get("provider", defaults.get("provider"))
    prompts_dir_str = config.prompts_dir
    prompts_dir = Path(prompts_dir_str) if prompts_dir_str else None

    # Stream the LLM node
    async for token in execute_prompt_streaming(
        prompt_name=prompt_name,
        variables=variables,
        temperature=temperature,
        provider=provider,
        graph_path=path,
        prompts_dir=prompts_dir,
        prompts_relative=config.prompts_relative,
        state=pre_state,
    ):
        yield token


__all__ = [
    "execute_prompt_async",
    "execute_prompt_streaming",
    "execute_prompts_concurrent",
    "run_graph_async",
    "run_graph_streaming",
    "compile_graph_async",
    "load_and_compile_async",
]
